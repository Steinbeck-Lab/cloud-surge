# from google.cloud import storage
# import json
# import os

# def chunks(lst, n):
#     for i in range(0, len(lst), n):
#         yield lst[i:i + n]

# r.delete('surge_jobs')
# r.delete('surge_jobs:processing')

# job_id=""
# print(job_id)

# pendingJobs = r.lrange('surge_jobs', 0, -1)
# pendingJobsCount = len(pendingJobs)
# print(pendingJobsCount)

# processingJobs = r.lrange('surge_jobs:processing', 0, -1)
# processingJobsCount = len(processingJobs)
# print("processing: " +str(processingJobsCount))
# # r.delete('surge_jobs:processing')

# jobId = job_id
# print("Job id:" + str(jobId))
# pendingJobs = r.lrange('surge_jobs', 0, -1)
# pendingJobsCount = len(pendingJobs)
# print("Pending mfs:" + str(pendingJobsCount))
# completedJobs = r.lrange(jobId + ':completed', 0, -1)
# completedJobsCount = len(completedJobs)
# print("Completed mfs:" + str(completedJobsCount))
# failedJobs = r.lrange(jobId + ':failed', 0, -1)
# # print(failedJobs)
# failedJobsCount = len(failedJobs)
# print("Failed mfs:" + str(failedJobsCount))
# processingJobsCount = totalJobs - (completedJobsCount + failedJobsCount + pendingJobsCount)
# print("Lease / processing mfs:" + str(processingJobsCount))


# # # Export output
# parsedKeys=[]
# exportJobId = str(job_id)
# # r.delete(exportJobId + ':failed')
# # r.delete(exportJobId + ':completed')
# jobOutputFile = open("logs/" + exportJobId + '.csv',"a+")
# with open("./formulae/formulaeMax13HA.txt") as f:
#     lines = f.readlines()
#     nkey = exportJobId
#     for mfs in chunks(lines, 500):
#         smfs = []
#         for mf in mfs:
#             if mf not in parsedKeys:
#                 smfs.append(nkey + ":" + mf.rstrip())
#         mfdata = r.mget(smfs)
#         for mfd in mfdata:
#             mObj = {}
#             if mfd:
#                 value = json.loads(mfd.decode("utf-8"))
#                 # print(value)
#                 sout = value['stdErr'].rstrip().replace('\n', '|').replace('\r', '|')
#                 mObj['mf'] = sout.split("  ")[0]
#                 mObj['s_totalStructuresCount'] = sout.split(" ")[-4]
#                 mObj['totalStructuresCount'] = str(value['totalStructuresCount'])
#                 mObj['oFileSize'] = str(value['oFileSize'])
#                 mObj['runtime'] = value['runtime']
#                 mObj['s_runtime'] = sout.split(" ")[-2]
#                 mObj['start'] = value['start']
#                 mObj['end'] = value['end']
#                 mObj['stdOut'] = sout
#                 mObj['stdErr'] = value['stdOut'].rstrip().replace('\n', '|').replace('\r', '|')
#                 jobOutputFile.write(",".join(list(mObj.values()))+'\n')
#                 # r.delete(key)
#                 parsedKeys.append(nkey)
#             else:
#                 print(mfd)
#         print(len(parsedKeys))
# # # delete parsed keys
# jobOutputFile.close()

# Delete all keys
# for key in r.scan_iter("*"):
#     print(r.delete(key))

# mf = []
# with open("./formulae/13HAonly.txt") as f:
#     lines = f.readlines()
#     for line in lines:
#         mf.append(line.rstrip())

# outMf = []
# with open("logs/" + exportJobId + '.csv') as f:
#     lines = f.readlines()
#     for line in lines:
#         imf = line.rstrip().split(",")[0]
#         outMf.append(imf)

# failedMF = list(set(outMf).symmetric_difference(set(mf)))

# print(len(mf) - len(outMf))
# print(len(failedMF))

# jobFailedOutputFile = open("formulae/" + exportJobId + '.txt',"a+")

# for fmf in failedMF:
#     jobFailedOutputFile.write(fmf+'\n')

bucket: steinbeck-surge-results

```
gsutil -m cp -r "gs://steinbeck-surge-results/<jobid>/" .
gsutil -m cp -r "gs://steinbeck-surge-results/1dc6de65-37a1-496b-a993-63e3d1414bd8/" .
```

**Commands**

Redis:

App
```
kubectl apply -f ./redis/redis-pod.yaml
```
Service
```
kubectl apply -f ./redis/redis-service.yaml
```
Port forwarding to access from cloudshell
```
kubectl port-forward redis-master 6379:6379
```

Building the worker container and pushing the image to google artifact registry

```
docker build -t surge-peq .
docker tag surge-peq us-central1-docker.pkg.dev/steffen-nfdi-spielplatz/surge-peqi-repository/surge-peq
docker push us-central1-docker.pkg.dev/steffen-nfdi-spielplatz/surge-peqi-repository/surge-peq
```

```
docker build -t surge-peq-ss .
docker tag surge-peq-ss us-central1-docker.pkg.dev/steffen-nfdi-spielplatz/surge-peqi-repository/surge-peq-ss
docker push us-central1-docker.pkg.dev/steffen-nfdi-spielplatz/surge-peqi-repository/surge-peq-ss
```

```
kubectl run -i --tty temp --image us-central1-docker.pkg.dev/steffen-nfdi-spielplatz/surge-peqi-repository/surge-peq:latest --command "/bin/sh"
```

Miscellaneous commands

```
kubectl exec --stdin --tty surge-job-wq-kftlf -- /bin/bash

kubectl describe jobs/surge-job-wq

kubectl apply -f ./job.yaml

kubectl apply -f ./secrets.yaml

kubectl delete pod/temp

kubectl delete jobs `kubectl get jobs -o custom-columns=:.metadata.name`

kubectl get pods --all-namespaces | grep Evicted | awk '{print $2, "--namespace", $1}' | xargs kubectl delete pod

kubectl get svc --all-namespaces -o json | jq '.items[] | {name:.metadata.name, ns:.metadata.namespace, p:.spec.ports[] } | select( .p.nodePort != null ) | "\(.ns)/\(.name): localhost:\(.p.nodePort) -> \(.p.port) -> \(.p.targetPort)"'
```

**Links**

https://kubernetes.io/docs/reference/kubectl/cheatsheet/

https://github.com/StructureGenerator/surge

https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/

https://github.com/sneumann/CloudSurge
