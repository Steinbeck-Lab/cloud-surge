# Sustainability

The endeavor to run generation of GDB on a cloud platform has similarities but also differences to doing so on e.g. an university HPC cluster. Not all university and even fewer departmental HPC installations will run under tight resource control, and will usually monitor in particular the CPU utilization. In this case, to run large computational jobs it is sufficient to optimize the algorithms and workflows until they successfully run on the HPC setup without failing due to e.g. RAM or storage limits. In case of bottlenecks, often more time or hardware is thrown at the problem. Nearly all cloud installations have fine-grained resource monitoring and accounting built in, which take into account all aspects from CPU time, memory, local storage, network traffic and non-ephemeral and non-local storage to name just a few. So it literally pays off to optimize the resource utilization and hence costs across the entire computational workflow. For example, additional CPU costs for compressing the output could be outweighed by the subsequent reduction for transport and storage. While at a first glance this seems like an undue burden on the developers and researchers, the costs charged by the cloud providers serve as a fine-grained proxy for the overall resource consumption. This highlights an approach and necessity that can not be underestimated today and even more so in the future: being resourceful, developing towards greener computing and reducing the energy and carbon footprint of science. 
